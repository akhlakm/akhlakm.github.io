{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0d02950",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Quantization of a large language model\"\n",
    "date: May 23, 2023\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "140e5a79",
   "metadata": {},
   "source": [
    "This is a test of [the AutoGPTQ quantization library](https://github.com/PanQiWei/AutoGPTQ). The pip version of the model is not always up to date. It is better to clone the git repository, and run `pip install .` from inside the cloned repo.\n",
    "Note: triton is available only on linux hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c11de27-42b1-4d5d-becb-55cd4340af4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install auto-gptq[triton]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d26a6b-3f94-4f0e-8cd2-95af0b6b0ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aad7e0d-58b8-4290-90ee-c1915c052758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d0b76e-a132-49a1-84ea-74fdb7db08d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60027441-3f0d-490d-9520-48508c5610ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"facebook/opt-125m\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbcb28e9",
   "metadata": {},
   "source": [
    "The default for `desc_act` is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc284ea-919a-4acf-847e-bb3ee5225291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(bits=4, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c60817-5c3d-48d7-a779-fc57de3fb80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_name, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72cb4b6c-3d10-449a-a2ef-4151e7f5023e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd891da6-aef1-4b67-9191-f54c13558685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 39545, 12, 571, 3320, 1343, 16, 41, 1365, 12, 560, 12, 3698, 1421, 24934, 1938, 5560, 19, 3018, 12, 6928, 6256, 354, 6, 716, 15, 272, 10311, 1864, 17194, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da35192f-c3ce-4624-8d3c-17f28c86d889",
   "metadata": {},
   "source": [
    "The following step takes care of the weight quantization and took about 10 mins. There is no training loops. The examples are used to measure model loss due to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9d8c25-8894-42e3-ab00-9ca5b0387da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.quantize(examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fdfae65-52ff-4b9f-81da-216f2ab7bb93",
   "metadata": {},
   "source": [
    "VRAM uses increases upto 1100 MiB during the quantization process. Probably because of triton. However, quantization should be possible using RAM only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4153f0a-d08b-452e-a537-2b797433c14e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "quantized_model_dir = \"opt-125m-4bit-128g\"\n",
    "os.makedirs(quantized_model_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36ded59b-7b62-4350-882e-e3cce51b97e9",
   "metadata": {},
   "source": [
    "Safetensors are supposed to be more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae6f847-18da-4556-8cfe-ab2bb7e631ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d900f9c4-26e4-4c4c-9323-8bcda0ad3916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb4de57-610d-479e-bf6e-ad5247a8acb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaf0d5e0-ce07-438b-8398-3ab7663c8348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d08868c-1f19-4c90-8e9e-d24ef8f9215f",
   "metadata": {},
   "source": [
    "Even after deleting the model and freeing up the cache, I don't see decrease in the VRAM usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dafb91fd-f267-479c-abd2-a5039020f7e9",
   "metadata": {},
   "source": [
    "The following step takes about 3 minutes. This is considerably slower than other methods I tested. Obabooga loads a bigger model almost within 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb79cc94-4baf-472c-9ff8-a3e6d2bf8059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 15:15:35 WARNING [auto_gptq.modeling._base] use_triton will force moving the whole model to GPU, make sure you have enough VRAM.\n",
      "2023-05-23 15:15:35 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.\n",
      "2023-05-23 15:15:35 WARNING [accelerate.utils.modeling] The safetensors archive passed at opt-125m-4bit-128g/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "2023-05-23 15:15:35 INFO [auto_gptq.nn_modules.qlinear_triton] Found 3 unique KN Linear values.\n",
      "2023-05-23 15:15:35 INFO [auto_gptq.nn_modules.qlinear_triton] Warming up autotune cache ...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [02:59<00:00, 14.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# load quantized model, currently only support cpu or single gpu\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                           device=\"cuda:0\",\n",
    "                                           use_triton=True,\n",
    "                                           use_safetensors=True,\n",
    "                                          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6d4327-71b2-48cc-909e-be1be9a58f60",
   "metadata": {},
   "source": [
    "As we can see, the inference does not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "927e0a15-9988-41ee-bc87-9c55ad7ff30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>auto_gptq is is is is is is is is is is is is is is\n"
     ]
    }
   ],
   "source": [
    "# inference with model.generate\n",
    "print(tokenizer.decode(\n",
    "    model.generate(**tokenizer(\n",
    "        \"auto_gptq is\", return_tensors=\"pt\").to(\"cuda:0\"))[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f40690",
   "metadata": {},
   "source": [
    "Overall, the package can be used to quantize a bigger model from HF. The inference capability should improve with the model size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
