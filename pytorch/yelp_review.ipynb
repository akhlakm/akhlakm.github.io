{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fcd399a-2718-4cc6-bf40-095b15370602",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- Yelp reviews about restaurants.\n",
    "- 1- and 2-star ratings are classified as \"negative\"\n",
    "- 3- and 4-star ratings are are \"positive\"\n",
    "- 560K training samples, 38K testing samples. We are using 10% here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448dea56-a182-4e1e-be47-e13f9d4a30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b06c578-6560-414c-ada7-e19591e305bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../../PyTorchNLPBook/data/yelp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2561b9ca-f29e-41e5-95e9-0b0f1d3d5a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>positive</td>\n",
       "      <td>great food . wonderful , friendly service . i ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>positive</td>\n",
       "      <td>charlotte should be the new standard for moder...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>positive</td>\n",
       "      <td>get the encore sandwich ! ! make sure to get i...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>positive</td>\n",
       "      <td>i m a pretty big ice cream gelato fan . pretty...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>positive</td>\n",
       "      <td>where else can you find all the parts and piec...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review  split\n",
       "0      negative  terrible place to work for i just heard a stor...  train\n",
       "1      negative   hours , minutes total time for an extremely s...  train\n",
       "2      negative  my less than stellar review is for service . w...  train\n",
       "3      negative  i m granting one star because there s no way t...  train\n",
       "4      negative  the food here is mediocre at best . i went aft...  train\n",
       "...         ...                                                ...    ...\n",
       "55995  positive  great food . wonderful , friendly service . i ...   test\n",
       "55996  positive  charlotte should be the new standard for moder...   test\n",
       "55997  positive  get the encore sandwich ! ! make sure to get i...   test\n",
       "55998  positive  i m a pretty big ice cream gelato fan . pretty...   test\n",
       "55999  positive  where else can you find all the parts and piec...   test\n",
       "\n",
       "[56000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_root + \"/reviews_with_splits_lite.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d67e1ddb-44cb-4831-8f07-eb80aa1cbe96",
   "metadata": {},
   "source": [
    "The dataset has been separated into 'test', 'train', and 'val' splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af94676-de8c-4b5c-b16d-0172c3eeec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'val', 'test'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.split.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb14dcb6-b6ae-430c-bbd1-4e02680766c2",
   "metadata": {},
   "source": [
    "The review column text has been processed to \n",
    "- make the text lowercase\n",
    "- adding spaces before and after the punctuations\n",
    "- replacing all other symbols with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f2c99b-e288-439d-8177-b5f3646799b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac84d5d-8521-4762-a17c-f2d81d9c7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        # create the inverse mapping\n",
    "        self._idx_to_token = { idx : token for token, idx in \\\n",
    "                                    self._token_to_idx.items() }\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" Add a token to mappings and return it's index. \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Get the index corresponding to a token \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" Get the token corresponding to an index. \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" %index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" %len(self)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self._unk_token\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83289876-96cb-4649-97d8-bb120cd6e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb10db13-838e-4cc5-a6fd-9b95c2fa070e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e52825-24a2-4745-bfed-0b9d49f4d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\" Returns a collapsed one-hot vector for a given review. \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                index = self.review_vocab.lookup_token(token)\n",
    "                one_hot[index] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        # Add ratings\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            'review_vocab': self.review_vocab.to_serializable(),\n",
    "            'rating_vocab': self.rating_vocab.to_serializable()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        return cls(review_vocab, rating_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ccd0bc-b6be-4644-a30a-8556ec80d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.val_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac8f72b-7379-4ef8-b61a-ce05ae29ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c884c389-3b58-43ff-9603-2204434db0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last = True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A minibatch generator function which wraps the PyTorch DataLoader.\n",
    "    It will ensure each tensor is on the right device.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de082ed0-71c5-4d43-811a-9ca5199a9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f8c83b-c89a-4210-9e47-01860ff6ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fcl = nn.Linear(in_features=num_features,\n",
    "                            out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        y_out = self.fcl(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c429479-df66-421c-8b41-54edd9caf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0cc0f2-3769-44ae-b11b-4cbe9e964a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    frequency_cutoff = 25,\n",
    "    model_state_file = 'model.pth',\n",
    "    review_csv = data_root + \"/reviews_with_splits_lite.csv\",\n",
    "    save_dir = \".\",\n",
    "    # No model hyperparams\n",
    "    # Training hypereparams\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=15,\n",
    "    seed=1337,\n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74837b3-65bb-45f2-8df5-4e973790cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018f9116-a1e2-4565-829e-9ac43e7a149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {\n",
    "        'epoch_index': 0,\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdcfad12-694b-4e99-ae44-a89e62db3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d8eee67-c58b-449d-84ff-30344f86dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3718fb-dcb4-4627-a903-927819bf18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d94dc8-6608-4e0c-9358-0141ef3e7cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f33139b-6ca7-4abb-8a8d-ab7ba65ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94fc8447-5804-4e10-b5dc-6f286219465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "798581ca-1a84-48c0-aa91-c413605625fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15efbb03-abd9-4b5d-8479-88cad52c2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c138a9f0-66f1-4a8d-83b0-62ab5c4ae2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98780aeb-357a-44dc-a833-9d56ee6c622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, target):\n",
    "    return ((target - pred) / target).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ce8531-28f4-46d5-9e2f-61a28ffb5f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(np.zeros(10), np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fcecbef-8eeb-4a61-920a-ec1898773b8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m running_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     13\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, batch_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_generator):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# zero the gradients\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# compute the output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mgenerate_batches\u001b[0;34m(dataset, batch_size, shuffle, drop_last, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mA minibatch generator function which wraps the PyTorch DataLoader.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mIt will ensure each tensor is on the right device.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      8\u001b[0m                      batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      9\u001b[0m                      shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m     10\u001b[0m                      drop_last\u001b[38;5;241m=\u001b[39mdrop_last)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_dict \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     13\u001b[0m     out_data_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m, in \u001b[0;36mReviewDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 46\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     47\u001b[0m     review_vector \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorizer\u001b[38;5;241m.\u001b[39mvectorize(row\u001b[38;5;241m.\u001b[39mreview)\n\u001b[1;32m     49\u001b[0m     rating_index \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorizer\u001b[38;5;241m.\u001b[39mrating_vocab\u001b[38;5;241m.\u001b[39mlookup_token(row\u001b[38;5;241m.\u001b[39mrating)\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/pandas/core/indexing.py:1658\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/pandas/core/frame.py:3652\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3650\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3652\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/pandas/core/internals/managers.py:1044\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miget((\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), loc))\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;66;03m# in the case of a single block, the new block is a view\u001b[39;00m\n\u001b[0;32m-> 1044\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1052\u001b[0m dtype \u001b[38;5;241m=\u001b[39m interleaved_dtype([blk\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks])\n",
      "File \u001b[0;32m~/miniconda3/envs/website/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2383\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim, refs)\u001b[0m\n\u001b[1;32m   2379\u001b[0m     values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(values)\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m klass(values, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, placement\u001b[38;5;241m=\u001b[39mplacement, refs\u001b[38;5;241m=\u001b[39mrefs)\n\u001b[0;32m-> 2383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_block\u001b[39m(\n\u001b[1;32m   2384\u001b[0m     values, placement, \u001b[38;5;241m*\u001b[39m, ndim: \u001b[38;5;28mint\u001b[39m, refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;66;03m# caller is responsible for ensuring values is NOT a PandasArray\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(placement, BlockPlacement):\n\u001b[1;32m   2389\u001b[0m         placement \u001b[38;5;241m=\u001b[39m BlockPlacement(placement)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# A bare-bones training loop\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # Iterate over training dataset\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    device=args.device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over validation dataset\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "412bbbb1-0a3f-48e4-ae6e-3089d6789bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch_index': 22,\n",
       " 'train_loss': [0.48078572292343463,\n",
       "  0.32938406645667323,\n",
       "  0.27444523850492386,\n",
       "  0.24338128739127932,\n",
       "  0.22270150414479334,\n",
       "  0.20760727328023101,\n",
       "  0.19577689098766424,\n",
       "  0.18615816270603858,\n",
       "  0.17823111704167202,\n",
       "  0.17131327121865514,\n",
       "  0.1652598268309839,\n",
       "  0.16008958478573887,\n",
       "  0.1552568533354335,\n",
       "  0.15096716351466236,\n",
       "  0.14698867341564376,\n",
       "  0.1434966386874128,\n",
       "  0.14009482598577452,\n",
       "  0.13711761412959464,\n",
       "  0.13425543120289157,\n",
       "  0.13144471408689717,\n",
       "  0.12905195598898375,\n",
       "  0.1264154611800622],\n",
       " 'train_acc': [tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)],\n",
       " 'val_loss': [0.3827091707633092,\n",
       "  0.3089083020503706,\n",
       "  0.2740706067818864,\n",
       "  0.25343966277746044,\n",
       "  0.2393623280983705,\n",
       "  0.22958406485044042,\n",
       "  0.2229269543519387,\n",
       "  0.21800954869160286,\n",
       "  0.2149605845029538,\n",
       "  0.2100421364490802,\n",
       "  0.20944883227348335,\n",
       "  0.20681792107912217,\n",
       "  0.2058340288125552,\n",
       "  0.2045778891214958,\n",
       "  0.20414738998963283,\n",
       "  0.2039427431730124,\n",
       "  0.2039891733573033,\n",
       "  0.20289532026419277,\n",
       "  0.20320990589948798,\n",
       "  0.20385687832648938,\n",
       "  0.2045557994108934,\n",
       "  0.20566382247668039],\n",
       " 'val_acc': [tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)],\n",
       " 'test_loss': -1,\n",
       " 'test_acc': -1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb11bdf2-1338-4090-a7a9-c661a91740d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8945])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict['x_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fba6352-14d4-4058-b890-ce43a91ca422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18eef402-4765-4b64-ac70-1d7211099e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8945"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset._vectorizer.review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e91cee5d-dfc2-44fa-8490-9b3820509900",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewClassifier(\n",
       "  (fcl): Linear(in_features=8945, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c9ef00d-7302-455d-bb26-fd0a730a51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    gc.collect()\n",
    "    y_pred = classifier(x_in = batch_dict['x_data'].float())\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc0430d3-c46c-4b42-832c-1808901feac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21400380661854376"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc4af4d1-b18a-469b-819b-f13e48cb606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d345d5d-0344-4484-aa9b-cc024c7e3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15ab986c-7df1-45c4-b9a9-ccc8ee4aeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold = 0.5):\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review)).cuda()\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7bd32e13-a310-4793-9408-0ad4e890f856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3eb879ef-0e40-4274-90a8-e675d3d47634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcl_weights = classifier.fcl.weight.detach()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1361d384-d3fa-4bd1-8a41-f5abebf66aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = torch.sort(fcl_weights, dim=0, descending = True)\n",
    "indices = indices.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3de2879e-f77b-4dbe-bb66-002718ebaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delicious\n",
      "fantastic\n",
      "pleasantly\n",
      "amazing\n",
      "vegas\n",
      "great\n",
      "yum\n",
      "excellent\n",
      "ngreat\n",
      "awesome\n",
      "yummy\n",
      "perfect\n",
      "love\n",
      "bomb\n",
      "chinatown\n",
      "deliciousness\n",
      "solid\n",
      "notch\n",
      "hooked\n",
      "nthank\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a1b28d8-4ca4-4ab6-9d3e-d65545cb035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "meh\n",
      "awful\n",
      "terrible\n",
      "rude\n",
      "tasteless\n",
      "overpriced\n",
      "disgusting\n",
      "slowest\n",
      "unacceptable\n",
      "poorly\n",
      "nmaybe\n",
      "unfriendly\n",
      "downhill\n",
      "disappointing\n",
      "disappointment\n",
      "underwhelmed\n"
     ]
    }
   ],
   "source": [
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9e2c5-794a-4f85-9e7b-5efe6b1d29a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
