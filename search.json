[
  {
    "objectID": "pytorch/activation_functions.html",
    "href": "pytorch/activation_functions.html",
    "title": "Activation functions",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pytorch/activation_functions.html#sigmoid-function",
    "href": "pytorch/activation_functions.html#sigmoid-function",
    "title": "Activation functions",
    "section": "Sigmoid function",
    "text": "Sigmoid function\n\nsigmoid = torch.nn.Sigmoid()\nx = torch.arange(-10, 10, step=1)\n\nplt.plot(x, sigmoid(x), 'k-', label = 'sigmoid')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.show()"
  },
  {
    "objectID": "pytorch/activation_functions.html#relu-function",
    "href": "pytorch/activation_functions.html#relu-function",
    "title": "Activation functions",
    "section": "ReLu function",
    "text": "ReLu function\n\nrelu = torch.nn.ReLU()\nx = torch.arange(-10, 10, step=1)\n\nplt.plot(x, relu(x), 'k-', label = 'ReLu')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.show()"
  },
  {
    "objectID": "pytorch/activation_functions.html#leaky-relu",
    "href": "pytorch/activation_functions.html#leaky-relu",
    "title": "Activation functions",
    "section": "Leaky ReLu",
    "text": "Leaky ReLu\n\nf = torch.nn.LeakyReLU(negative_slope=0.1)\nx = torch.arange(-10, 10, step=1.0)\n\nplt.plot(x, f(x), 'k-', label = 'Leaky ReLu')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.show()"
  },
  {
    "objectID": "pytorch/activation_functions.html#softmax-function",
    "href": "pytorch/activation_functions.html#softmax-function",
    "title": "Activation functions",
    "section": "Softmax function",
    "text": "Softmax function\n\n# create a softmax module to apply to the last dimension of the input tensor\nf = torch.nn.Softmax(dim=-1)\nx = torch.arange(-10, 10, step=1.0)\n\nplt.plot(x, f(x), 'k-', label = 'Softmax')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.show()\n\n\n\n\nSoftmax is useful for a vector input. The sum of the values for each element of the vector becomes 1.0\n\n# create a vector (rank 2 tensor) with 3 elements\nx = torch.randn(1, 3)\nx\n\ntensor([[-0.4259,  1.3028, -1.4466]])\n\n\n\n# apply softmax along the last dim (column)\nf = torch.nn.Softmax(dim=1)\ny = f(x)\ny\n\ntensor([[0.1430, 0.8055, 0.0515]])\n\n\n\n# verify the sum of the softmax values\ntorch.sum(y, dim=1)\n\ntensor([1.])"
  },
  {
    "objectID": "pytorch/yelp_review.html",
    "href": "pytorch/yelp_review.html",
    "title": "Dataset",
    "section": "",
    "text": "Yelp reviews about restaurants.\n1- and 2-star ratings are classified as “negative”\n3- and 4-star ratings are are “positive”\n560K training samples, 38K testing samples. We are using 10% here.\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndata_root = \"../../PyTorchNLPBook/data/yelp\"\n\n\ndf = pd.read_csv(data_root + \"/reviews_with_splits_lite.csv\")\ndf\n\n\n\n\n\n\n\n\nrating\nreview\nsplit\n\n\n\n\n0\nnegative\nterrible place to work for i just heard a stor...\ntrain\n\n\n1\nnegative\nhours , minutes total time for an extremely s...\ntrain\n\n\n2\nnegative\nmy less than stellar review is for service . w...\ntrain\n\n\n3\nnegative\ni m granting one star because there s no way t...\ntrain\n\n\n4\nnegative\nthe food here is mediocre at best . i went aft...\ntrain\n\n\n...\n...\n...\n...\n\n\n55995\npositive\ngreat food . wonderful , friendly service . i ...\ntest\n\n\n55996\npositive\ncharlotte should be the new standard for moder...\ntest\n\n\n55997\npositive\nget the encore sandwich ! ! make sure to get i...\ntest\n\n\n55998\npositive\ni m a pretty big ice cream gelato fan . pretty...\ntest\n\n\n55999\npositive\nwhere else can you find all the parts and piec...\ntest\n\n\n\n\n56000 rows × 3 columns\n\n\n\nThe dataset has been separated into ‘test’, ‘train’, and ‘val’ splits.\n\ndf.split.unique()\n\narray(['train', 'val', 'test'], dtype=object)\n\n\nThe review column text has been processed to - make the text lowercase - adding spaces before and after the punctuations - replacing all other symbols with spaces\n\nfrom torch.utils.data import Dataset\n\n\nclass Vocabulary(object):\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token='&lt;UNK&gt;'):\n        if token_to_idx is None:\n            token_to_idx = {}\n        self._token_to_idx = token_to_idx\n\n        # create the inverse mapping\n        self._idx_to_token = { idx : token for token, idx in \\\n                                    self._token_to_idx.items() }\n\n        self._add_unk = add_unk\n        self._unk_token = unk_token\n\n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token)\n\n    def add_token(self, token):\n        \"\"\" Add a token to mappings and return it's index. \"\"\"\n        if token in self._token_to_idx:\n            index = self._token_to_idx[token]\n        else:\n            index = len(self._token_to_idx)\n            self._token_to_idx[token] = index\n            self._idx_to_token[index] = token\n        return index\n\n    def lookup_token(self, token):\n        \"\"\" Get the index corresponding to a token \"\"\"\n        if self._add_unk:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]\n\n    def lookup_index(self, index):\n        \"\"\" Get the token corresponding to an index. \"\"\"\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" %index)\n        return self._idx_to_token[index]\n\n    def __len__(self):\n        return len(self._token_to_idx)\n\n    def __str__(self):\n        return \"&lt;Vocabulary(size=%d)&gt;\" %len(self)\n\n    def to_serializable(self):\n        return {\n            'token_to_idx': self._token_to_idx,\n            'add_unk': self._add_unk,\n            'unk_token': self._unk_token\n        }\n\n    @classmethod\n    def from_serializable(cls, contents):\n        return cls(**contents)    \n\n\nimport numpy as np\nimport string\nfrom collections import Counter\n\n\nstring.punctuation\n\n'!\"#$%&\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~'\n\n\n\nclass ReviewVectorizer(object):\n    def __init__(self, review_vocab, rating_vocab):\n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab\n\n    def vectorize(self, review):\n        \"\"\" Returns a collapsed one-hot vector for a given review. \"\"\"\n        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n        for token in review.split(\" \"):\n            if token not in string.punctuation:\n                index = self.review_vocab.lookup_token(token)\n                one_hot[index] = 1\n        return one_hot\n\n    @classmethod\n    def from_dataframe(cls, review_df, cutoff=25):\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n\n        # Add ratings\n        for rating in sorted(set(review_df.rating)):\n            rating_vocab.add_token(rating)\n\n        word_counts = Counter()\n        for review in review_df.review:\n            for word in review.split(\" \"):\n                if word not in string.punctuation:\n                    word_counts[word] += 1\n\n        for word, count in word_counts.items():\n            if count &gt; cutoff:\n                review_vocab.add_token(word)\n\n        return cls(review_vocab, rating_vocab)\n\n    def to_serializable(self):\n        return {\n            'review_vocab': self.review_vocab.to_serializable(),\n            'rating_vocab': self.rating_vocab.to_serializable()\n        }\n\n    @classmethod\n    def from_serializable(cls, contents):\n        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n        return cls(review_vocab, rating_vocab)\n\n\nclass ReviewDataset(Dataset):\n    def __init__(self, review_df, vectorizer):\n        self.review_df = review_df\n        self._vectorizer = vectorizer\n\n        self.train_df = self.review_df[self.review_df.split == 'train']\n        self.val_df = self.review_df[self.review_df.split == 'val']\n        self.test_df = self.review_df[self.review_df.split == 'test']\n\n        self.train_size = len(self.train_df)\n        self.val_size = len(self.val_df)\n        self.test_size = len(self.test_df)\n\n        self._lookup_dict = {\n            'train': (self.train_df, self.train_size),\n            'val': (self.val_df, self.val_size),\n            'test': (self.test_df, self.test_size)\n        }\n\n        self.set_split('train')\n\n    @classmethod\n    def load_dataset_and_make_vectorizer(cls, review_csv):\n        \"\"\"Load dataset and make a new vectorizer from scratch.\n\n        Args:\n            review_csv (str): location of the dataset\n        Returns:\n            an instance of ReviewDataset\n        \"\"\"\n\n        review_df = pd.read_csv(review_csv)\n        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n\n    def get_vectorizer(self):\n        return self._vectorizer\n\n    def set_split(self, split='train'):\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._target_size\n\n    def __getitem__(self, index):\n        row = self._target_df.iloc[index]\n        review_vector = \\\n            self._vectorizer.vectorize(row.review)\n        rating_index = \\\n            self._vectorizer.rating_vocab.lookup_token(row.rating)\n        return {'x_data': review_vector,\n                'y_target': rating_index}\n\n    def get_num_batches(self, batch_size):\n        return len(self) // batch_size\n\n\nfrom torch.utils.data import DataLoader\n\n\ndef generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last = True, device=\"cpu\"):\n    \"\"\"\n    A minibatch generator function which wraps the PyTorch DataLoader.\n    It will ensure each tensor is on the right device.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset,\n                         batch_size=batch_size,\n                         shuffle=shuffle,\n                         drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n        yield out_data_dict\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ReviewClassifier(nn.Module):\n    def __init__(self, num_features):\n        \"\"\"\n        Args:\n            num_features (int): the size of the input feature vector\n        \"\"\"\n        super(ReviewClassifier, self).__init__()\n        self.fcl = nn.Linear(in_features=num_features,\n                            out_features=1)\n\n    def forward(self, x_in, apply_sigmoid=False):\n        y_out = self.fcl(x_in).squeeze()\n        if apply_sigmoid:\n            y_out = F.sigmoid(y_out)\n        return y_out\n\n\nfrom argparse import Namespace\n\n\nargs = Namespace(\n    # Data and path information\n    frequency_cutoff = 25,\n    model_state_file = 'model.pth',\n    review_csv = data_root + \"/reviews_with_splits_lite.csv\",\n    save_dir = \".\",\n    # No model hyperparams\n    # Training hypereparams\n    batch_size=128,\n    early_stopping_criteria=5,\n    learning_rate=0.001,\n    num_epochs=15,\n    seed=1337,\n    cuda=True\n)\n\n\nimport torch.optim as optim\n\n\ndef make_train_state(args):\n    return {\n        'epoch_index': 0,\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': [],\n        'test_loss': -1,\n        'test_acc': -1,\n    }\n\n\ntrain_state = make_train_state(args)\n\n\nimport torch\n\n\nif not torch.cuda.is_available():\n    args.cuda = False\n\n\nargs.cuda\n\nTrue\n\n\n\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n\n\nargs.device\n\ndevice(type='cuda')\n\n\n\ndataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\nvectorizer = dataset.get_vectorizer()\n\n\nclassifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\nclassifier = classifier.to(args.device)\n\n\nloss_func = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)\n\n\ndef compute_accuracy(pred, target):\n    return ((target - pred) / target).sum()\n\n\ncompute_accuracy(np.zeros(10), np.ones(10))\n\n10.0\n\n\n\n# A bare-bones training loop\nfor epoch_index in range(args.num_epochs):\n    train_state['epoch_index'] = epoch_index\n\n    # Iterate over training dataset\n    dataset.set_split('train')\n    batch_generator = generate_batches(dataset,\n                                    batch_size=args.batch_size,\n                                    device=args.device)\n\n    running_loss = 0.0\n    running_acc = 0.0\n    classifier.train()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n        # zero the gradients\n        optimizer.zero_grad()\n\n        # compute the output\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n        # compute the loss\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n\n        # compute gradients\n        loss.backward()\n\n        # update params\n        optimizer.step()\n\n        # compute accuracy\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n\n\n    train_state['train_loss'].append(running_loss)\n    train_state['train_acc'].append(running_acc)\n\n    # Iterate over validation dataset\n    dataset.set_split('val')\n    batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n                                        device=args.device)\n    running_loss = 0.0\n    running_acc = 0.0\n    classifier.eval()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n        # compute the output\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n        # compute the loss\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n\n        # compute the accuracy\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n\n\n    train_state['val_loss'].append(running_loss)\n    train_state['val_acc'].append(running_acc)\n\nKeyboardInterrupt: \n\n\n\ntrain_state\n\n{'epoch_index': 22,\n 'train_loss': [0.48078572292343463,\n  0.32938406645667323,\n  0.27444523850492386,\n  0.24338128739127932,\n  0.22270150414479334,\n  0.20760727328023101,\n  0.19577689098766424,\n  0.18615816270603858,\n  0.17823111704167202,\n  0.17131327121865514,\n  0.1652598268309839,\n  0.16008958478573887,\n  0.1552568533354335,\n  0.15096716351466236,\n  0.14698867341564376,\n  0.1434966386874128,\n  0.14009482598577452,\n  0.13711761412959464,\n  0.13425543120289157,\n  0.13144471408689717,\n  0.12905195598898375,\n  0.1264154611800622],\n 'train_acc': [tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)],\n 'val_loss': [0.3827091707633092,\n  0.3089083020503706,\n  0.2740706067818864,\n  0.25343966277746044,\n  0.2393623280983705,\n  0.22958406485044042,\n  0.2229269543519387,\n  0.21800954869160286,\n  0.2149605845029538,\n  0.2100421364490802,\n  0.20944883227348335,\n  0.20681792107912217,\n  0.2058340288125552,\n  0.2045778891214958,\n  0.20414738998963283,\n  0.2039427431730124,\n  0.2039891733573033,\n  0.20289532026419277,\n  0.20320990589948798,\n  0.20385687832648938,\n  0.2045557994108934,\n  0.20566382247668039],\n 'val_acc': [tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n  tensor(nan, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)],\n 'test_loss': -1,\n 'test_acc': -1}\n\n\n\nbatch_dict['x_data'].shape\n\ntorch.Size([128, 8945])\n\n\n\nimport gc\n\n\nlen(dataset._vectorizer.review_vocab)\n\n8945\n\n\n\nclassifier\n\nReviewClassifier(\n  (fcl): Linear(in_features=8945, out_features=1, bias=True)\n)\n\n\n\ndataset.set_split('test')\nbatch_generator = generate_batches(dataset, batch_size=args.batch_size,\n                                   device=args.device)\n\nrunning_loss = 0.0\nrunning_acc = 0.0\n\nclassifier.eval()\n\nfor batch_index, batch_dict in enumerate(batch_generator):\n    gc.collect()\n    y_pred = classifier(x_in = batch_dict['x_data'].float())\n    loss = loss_func(y_pred, batch_dict['y_target'].float())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n\ntrain_state['test_loss'] = running_loss\n\n\ntrain_state['test_loss']\n\n0.21400380661854376\n\n\n\nimport re\n\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n    return text\n\n\ndef predict_rating(review, classifier, vectorizer, decision_threshold = 0.5):\n    review = preprocess_text(review)\n    vectorized_review = torch.tensor(vectorizer.vectorize(review)).cuda()\n    result = classifier(vectorized_review.view(1, -1))\n    probability_value = torch.sigmoid(result).item()\n\n    index = 1\n    if probability_value &lt; decision_threshold:\n        index = 0\n\n    return vectorizer.rating_vocab.lookup_index(index)\n\n\ntest_review = \"this is a pretty awesome book\"\nprediction = predict_rating(test_review, classifier, vectorizer)\nprediction\n\n'positive'\n\n\n\nfcl_weights = classifier.fcl.weight.detach()[0]\n\n\n_, indices = torch.sort(fcl_weights, dim=0, descending = True)\nindices = indices.cpu().numpy().tolist()\n\n\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))\n\ndelicious\nfantastic\npleasantly\namazing\nvegas\ngreat\nyum\nexcellent\nngreat\nawesome\nyummy\nperfect\nlove\nbomb\nchinatown\ndeliciousness\nsolid\nnotch\nhooked\nnthank\n\n\n\nindices.reverse()\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))\n\nworst\nmediocre\nbland\nhorrible\nmeh\nawful\nterrible\nrude\ntasteless\noverpriced\ndisgusting\nslowest\nunacceptable\npoorly\nnmaybe\nunfriendly\ndownhill\ndisappointing\ndisappointment\nunderwhelmed\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pytorch/check_pytorch_cuda.html",
    "href": "pytorch/check_pytorch_cuda.html",
    "title": "Check PyTorch installation",
    "section": "",
    "text": "Import pytorch and check if cuda is available.\n\nimport torch\ntorch.cuda.is_available()\n\nTrue\n\n\nPytorch will be able to convert to cuda tensor if (a) GPU is available, (b) the installed version of pytorch was compiled for GPU. In my case, the conda installation failed, pip3 installation instruction provided in the pytorch website worked.\n\ntorch.zeros(10).cuda()\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Large language models/dolly-v2.html",
    "href": "Large language models/dolly-v2.html",
    "title": "Akhlak Mahmood",
    "section": "",
    "text": "import torch\nfrom transformers import pipeline\n\n/home/akhlak/micromamba/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nllm = pipeline(model=\"databricks/dolly-v2-3b\", device=0, trust_remote_code=True)\n\n\nllm(\"def print_hello(txt):\")\n\n[{'generated_text': 'Hello, world!'}]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Large language models/AutoGPTQ_test.html",
    "href": "Large language models/AutoGPTQ_test.html",
    "title": "Quantization of a large language model",
    "section": "",
    "text": "This is a test of the AutoGPTQ quantization library. The pip version of the model is not always up to date. It is better to clone the git repository, and run pip install . from inside the cloned repo. Note: triton is available only on linux hosts.\n\n#!pip install auto-gptq[triton]\n\n\nimport logging\nlogging.basicConfig(\n        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n\nfrom transformers import AutoTokenizer\n\n\npretrained_model_name = \"facebook/opt-125m\"\n\nThe default for desc_act is True.\n\nquantize_config = BaseQuantizeConfig(bits=4, group_size=128)\n\n\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_name, quantize_config)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n\n\nexamples = [\n    tokenizer(\n        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n    )\n]\n\n\nexamples[0]\n\n{'input_ids': [2, 39545, 12, 571, 3320, 1343, 16, 41, 1365, 12, 560, 12, 3698, 1421, 24934, 1938, 5560, 19, 3018, 12, 6928, 6256, 354, 6, 716, 15, 272, 10311, 1864, 17194, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nThe following step takes care of the weight quantization and took about 10 mins. There is no training loops. The examples are used to measure model loss due to quantization.\n\nmodel.quantize(examples)\n\nVRAM uses increases upto 1100 MiB during the quantization process. Probably because of triton. However, quantization should be possible using RAM only.\n\nimport os\nquantized_model_dir = \"opt-125m-4bit-128g\"\nos.makedirs(quantized_model_dir, exist_ok=True)\n\nSafetensors are supposed to be more memory efficient.\n\nmodel.save_quantized(quantized_model_dir, use_safetensors=True)\n\n\ndel model\n\n\nimport torch\n\n\ntorch.cuda.empty_cache()\n\nEven after deleting the model and freeing up the cache, I don’t see decrease in the VRAM usage.\nThe following step takes about 3 minutes. This is considerably slower than other methods I tested. Obabooga loads a bigger model almost within 10 seconds.\n\n# load quantized model, currently only support cpu or single gpu\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n                                           device=\"cuda:0\",\n                                           use_triton=True,\n                                           use_safetensors=True,\n                                          )\n\n2023-05-23 15:15:35 WARNING [auto_gptq.modeling._base] use_triton will force moving the whole model to GPU, make sure you have enough VRAM.\n2023-05-23 15:15:35 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.\n2023-05-23 15:15:35 WARNING [accelerate.utils.modeling] The safetensors archive passed at opt-125m-4bit-128g/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n2023-05-23 15:15:35 INFO [auto_gptq.nn_modules.qlinear_triton] Found 3 unique KN Linear values.\n2023-05-23 15:15:35 INFO [auto_gptq.nn_modules.qlinear_triton] Warming up autotune cache ...\n100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [02:59&lt;00:00, 14.96s/it]\n\n\nAs we can see, the inference does not work properly.\n\n# inference with model.generate\nprint(tokenizer.decode(\n    model.generate(**tokenizer(\n        \"auto_gptq is\", return_tensors=\"pt\").to(\"cuda:0\"))[0]))\n\n&lt;/s&gt;auto_gptq is is is is is is is is is is is is is is\n\n\nOverall, the package can be used to quantize a bigger model from HF. The inference capability should improve with the model size.\n\n\n\n Back to top"
  },
  {
    "objectID": "Large language models/finetune_model.html",
    "href": "Large language models/finetune_model.html",
    "title": "Finetuning a model with Transformers",
    "section": "",
    "text": "import torch\nimport evaluate\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification\n\n\ncheckpoint = \"bert-base-uncased\"\nraw_datasets = load_dataset('glue', 'mrpc')\n\nFound cached dataset glue (/home/akhlak/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|██████████| 3/3 [00:00&lt;00:00, 907.92it/s]\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(subset):\n    return tokenizer(subset['sentence1'], subset['sentence2'],\n                     truncation=True)\n\n\ntokeninzed_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nLoading cached processed dataset at /home/akhlak/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-000c138c4db1edcb.arrow\nLoading cached processed dataset at /home/akhlak/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ae70e2242c4dfe9f.arrow\n\n\n\n# Update dataset for model, the classes should be in column 'labels'\ntokeninzed_datasets = tokeninzed_datasets.rename_column(\"label\", \"labels\")\n\n# Remove original raw columns\ntokeninzed_datasets = tokeninzed_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n\n# Return pytorch tensors, not python lists\ntokeninzed_datasets.set_format(\"torch\")\n\n\ntrain_dataloader = DataLoader(tokeninzed_datasets['train'],\n                              shuffle=True, batch_size=8,\n                              collate_fn=data_collator)\neval_dataloader = DataLoader(tokeninzed_datasets['validation'],\n                             batch_size=8, collate_fn=data_collator)\n\n\n# Check dataloader batch\nfor batch in train_dataloader:\n    break\n{k : v.shape for k, v in batch.items()}\n\n{'labels': torch.Size([8]),\n 'input_ids': torch.Size([8, 70]),\n 'token_type_ids': torch.Size([8, 70]),\n 'attention_mask': torch.Size([8, 70])}\n\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# Check model is working\noutputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)\n\ntensor(0.7214, grad_fn=&lt;NllLossBackward0&gt;) torch.Size([8, 2])\n\n\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n/home/akhlak/micromamba/envs/pytorch/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n                             num_warmup_steps=0,\n                             num_training_steps=num_training_steps)\nprint(num_training_steps)\n\n1377\n\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n\ndevice(type='cuda')\n\n\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = { k : v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n100%|██████████| 1377/1377 [02:05&lt;00:00, 11.88it/s]\n\n\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmodel.eval()\n\nfor batch in eval_dataloader:\n    batch = {k : v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch['labels'])\n\nmetric.compute()\n\nDownloading builder script: 100%|██████████| 5.75k/5.75k [00:00&lt;00:00, 4.16MB/s]\n\n\n{'accuracy': 0.8676470588235294, 'f1': 0.9072164948453608}\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akhlak Mahmood",
    "section": "",
    "text": "Hello and welcome to my website! I am a postdoctoral researcher at Georgia Tech with expertise in polymer informatics. My research focuses on developing new and innovative polymer-based devices using a combination of computational modeling and natural language processing (NLP).\nMy PhD research at NC State focused on developing innovative modeling methods for ultra-small colloidal and magnetic nanoparticles. In addition, I applied machine learning and nanoinformatics to optimize external magnetic field guided self-assembly of nanomaterials.\nMy expertise include, but are not limited to:\n\nDeveloping novel methods for research related to nanotechnology and polymer informatics\nDeveloping custom NLP tools and software for data extraction, analysis and predictive modeling\nCollaborating on grant proposals and research projects\n\nI am dedicated to advancing the fields of nanotechnology and polymer informatics, and I am passionate about using cutting-edge technology to develop new and innovative solutions.\nThank you for visiting my website! Please feel free to contact me to discuss how I can assist you in your research endeavors.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "In the field of polymer informatics, I use NLP to perform data parsing, information extraction, database development, and knowledge curation. By leveraging the power of large language models, I am able to extract valuable insights from large amounts of data and use them to develop new and innovative polymer-based devices.\nIn the field of nanotechnology, I specialize in developing innovative modeling methods for ultra-small nanoparticles and applying machine learning and nanoinformatics to optimize external magnetic field guided self-assembly of nanomaterials. Additionally, I am proficient in large-scale atomistic simulations and molecular modeling techniques for magnetic and plasmonic nanoparticles.\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact",
    "section": "",
    "text": "Connect me via LinkedIn\nMy Google Scholar Profile\nFollow me on ResearchGate\n\n\n\n\n Back to top"
  }
]